{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from BERT import *\n",
    "from data import *\n",
    "from utils.remi import *\n",
    "from utils.utils import *\n",
    "from utils.vocab import *\n",
    "\n",
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load config file\n",
    "with open(\"./config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "### fix random seed\n",
    "random_seed = config[\"random_seed\"]\n",
    "\n",
    "# it may slow computing performance\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "#### initialize model with GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define model\n",
    "model = BERT_Lightning(\n",
    "    dim=config[\"dim\"],\n",
    "    depth=config[\"depth\"],\n",
    "    heads=config[\"heads\"],\n",
    "    dim_head=int(config[\"dim\"] / config[\"heads\"]),\n",
    "    mlp_dim=int(4 * config[\"dim\"]),\n",
    "    max_len=config[\"max_len\"],\n",
    "    rate=config[\"rate\"],\n",
    "    loss_weights=config[\"loss_weights\"],\n",
    "    lr=config[\"lr\"],\n",
    "    warm_up=config[\"warm_up\"],\n",
    "    temp=config[\"temp\"],\n",
    "    mode=config[\"mode\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/workspace/loop_season_2/model\"\n",
    "model_path = \"BERT-random_seed_0-batch_size_24-num_workers_16-dim_768-depth_12-heads_12-max_len_512-rate_0-masking_0.8-replace_0.1-loss_weights_[1, 0.1]-lr_0.0001-epochs_3-warm_up_10000-temp_0.1-gpus_[2, 3, 4, 5, 6, 7]-mode_BERT-neighbor-epoch=2-val_loss=0.7468.ckpt\"\n",
    "\n",
    "model = model.load_from_checkpoint(\n",
    "    os.path.join(folder_path, model_path),\n",
    "    dim=config[\"dim\"],\n",
    "    depth=config[\"depth\"],\n",
    "    heads=config[\"heads\"],\n",
    "    dim_head=int(config[\"dim\"] / config[\"heads\"]),\n",
    "    mlp_dim=int(4 * config[\"dim\"]),\n",
    "    max_len=config[\"max_len\"],\n",
    "    rate=config[\"rate\"],\n",
    "    loss_weights=config[\"loss_weights\"],\n",
    "    lr=config[\"lr\"],\n",
    "    warm_up=config[\"warm_up\"],\n",
    "    temp=config[\"temp\"],\n",
    "    mode=config[\"mode\"],\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "mode = model_path.split(\"mode_\")[1].split(\"-epoch\")[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"./data/lmd_full_remi/\"\n",
    "folder_list = glob.glob(os.path.join(folder_path, \"*\"))\n",
    "\n",
    "train_folder, val_folder, test_folder = dataset_split(folder_list)\n",
    "\n",
    "task = {\n",
    "    \"inst\": \"multi-label\",\n",
    "    \"chord\": \"multi-label\",\n",
    "    \"tempo\": \"multi-class\",\n",
    "    \"mean_velocity\": \"regression\",\n",
    "    \"mean_duration\": \"regression\",\n",
    "    \"groove_pattern\": \"multi-label\",\n",
    "}\n",
    "\n",
    "metrics = {}\n",
    "for i in range(5):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_files = folder_to_multiple_file(train_folder, k=5)\n",
    "    test_files = folder_to_multiple_file(test_folder, k=10)\n",
    "\n",
    "    random.shuffle(train_files)\n",
    "\n",
    "    ### load dataloader\n",
    "    train_module = DataModule(\n",
    "        train_files,\n",
    "        batch_size=config[\"batch_size\"] * 8,\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        masking=config[\"masking\"],\n",
    "        replace=config[\"replace\"],\n",
    "        phase=\"train\",\n",
    "    )\n",
    "\n",
    "    test_module = DataModule(\n",
    "        test_files,\n",
    "        batch_size=config[\"batch_size\"] * 8,\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        phase=\"test\",\n",
    "    )\n",
    "\n",
    "    train_set = train_module.return_dataloader()\n",
    "    test_set = test_module.return_dataloader()\n",
    "\n",
    "    train_feat, train_labels = get_feat(train_set, model, device)\n",
    "    test_feat, test_labels = get_feat(test_set, model, device)\n",
    "\n",
    "    metrics = probing(\n",
    "        (np.vstack(train_feat[11]), train_labels),\n",
    "        (np.vstack(test_feat[11]), test_labels),\n",
    "        metrics,\n",
    "        task,\n",
    "    )\n",
    "\n",
    "    metrics = clustering(\n",
    "        (np.vstack(train_feat[11]), train_labels[\"file_name\"]),\n",
    "        (np.vstack(test_feat[11]), test_labels[\"file_name\"]),\n",
    "        metrics,\n",
    "        num_k=1000,\n",
    "    )\n",
    "\n",
    "    print(f\"{i+1} iter : {time.time() - start_time:.3f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open(os.path.join(\"./results/\", mode + \"_results.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(metrics, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean and std of metrics\n",
    "metrics_mean_std = dict(zip(metrics.keys(), map(lambda x: (np.mean(x), np.std(x)), metrics.values())))\n",
    "\n",
    "print(metrics_mean_std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Probing for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "for i in range(12):\n",
    "    start_time = time.time()\n",
    "\n",
    "    metrics = probing(\n",
    "        (np.vstack(train_feat[i]), train_labels),\n",
    "        (np.vstack(test_feat[i]), test_labels),\n",
    "        metrics,\n",
    "        task,\n",
    "    )\n",
    "\n",
    "    metrics = clustering(\n",
    "        (np.vstack(train_feat[i]), train_labels[\"file_name\"]),\n",
    "        (np.vstack(test_feat[i]), test_labels[\"file_name\"]),\n",
    "        metrics,\n",
    "        num_k=1000,\n",
    "    )\n",
    "\n",
    "    print(f\"{i+1} layer : {time.time() - start_time:.3f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open(os.path.join(\"./results/\", mode + \"_results_each_layer.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(metrics, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = {\n",
    "    \"inst\": \"max\",\n",
    "    \"chord\": \"max\",\n",
    "    \"tempo\": \"max\",\n",
    "    \"mean_velocity\": \"min\",\n",
    "    \"mean_duration\": \"min\",\n",
    "    \"groove_pattern\": \"max\",\n",
    "    \"song_clustering\": \"min\",\n",
    "}\n",
    "\n",
    "for k, v in metrics.items():\n",
    "    if task[k] == \"max\":\n",
    "        layer = np.argmax(v) + 1\n",
    "        value = np.max(v)\n",
    "    elif task[k] == \"min\":\n",
    "        layer = np.argmin(v) + 1\n",
    "        value = np.min(v)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    print(f\"{k}:, {layer, value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_1_label = [\"inst\", \"chord\", \"tempo\", \"groove_pattern\", \"song_clustering\"]\n",
    "fig_2_label = [\"mean_velocity\", \"mean_duration\"]\n",
    "\n",
    "FIG_SIZE = (26, 4)\n",
    "NUM_FONT_SIZE = 16\n",
    "CHAR_FONT_SIZE = 18\n",
    "LABEL_PAD = 5\n",
    "\n",
    "TICKS = np.arange(0, 12)\n",
    "SAVE_PATH = \"./images/\"\n",
    "\n",
    "plt.figure(figsize=FIG_SIZE)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.text(-4.8, 0.45, mode, fontsize=CHAR_FONT_SIZE + 6)\n",
    "for label in fig_1_label:\n",
    "    plt.plot(metrics[label], \"-o\")\n",
    "plt.xticks(TICKS, labels=TICKS + 1, fontsize=NUM_FONT_SIZE)\n",
    "plt.yticks(fontsize=NUM_FONT_SIZE)\n",
    "plt.xlabel(\"BERT Layer\", fontsize=CHAR_FONT_SIZE, labelpad=LABEL_PAD)\n",
    "plt.ylabel(\"Performance\", fontsize=CHAR_FONT_SIZE, labelpad=LABEL_PAD)\n",
    "plt.legend(fig_1_label, fontsize=CHAR_FONT_SIZE - 4)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for label in fig_2_label:\n",
    "    plt.plot(metrics[label], \"-o\")\n",
    "plt.xticks(TICKS, labels=TICKS + 1, fontsize=NUM_FONT_SIZE)\n",
    "plt.yticks(fontsize=NUM_FONT_SIZE)\n",
    "plt.xlabel(\"BERT Layer\", fontsize=CHAR_FONT_SIZE, labelpad=LABEL_PAD)\n",
    "plt.ylabel(\"Performance\", fontsize=CHAR_FONT_SIZE, labelpad=LABEL_PAD)\n",
    "plt.legend(fig_2_label, fontsize=CHAR_FONT_SIZE - 4)\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout(w_pad=5.2)\n",
    "\n",
    "path = os.path.join(SAVE_PATH, mode + \".pdf\")\n",
    "plt.savefig(path, dpi=1000, bbox_inches=\"tight\", format=\"pdf\", pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "x = np.vstack(test_feat[0])\n",
    "x_embed = TSNE(n_components=2).fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = []\n",
    "for batch_idx, batch in enumerate(val_set):\n",
    "    file_list.append(batch[\"file_name\"])\n",
    "\n",
    "file_list = [elem for sublist in file_list for elem in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(x_embed[:, 0], x_embed[:, 1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
